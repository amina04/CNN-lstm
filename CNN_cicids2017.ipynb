{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amina04/CNN-lstm/blob/main/CNN_cicids2017.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xRw79tU1bYtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9afe962-8910-4dd1-a424-2abacad16df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F1p24aiQe_8Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd  \n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3kzVMujHfD_W"
      },
      "outputs": [],
      "source": [
        "d1=pd.read_csv('/content/drive/MyDrive/data/cicids-2017/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
        "d2=pd.read_csv('/content/drive/MyDrive/data/cicids-2017/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\n",
        "d3=pd.read_csv('/content/drive/MyDrive/data/cicids-2017/Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
        "d4=pd.read_csv('/content/drive/MyDrive/data/cicids-2017/Monday-WorkingHours.pcap_ISCX.csv')\n",
        "d5=pd.read_csv('/content/drive/MyDrive/data/cicids-2017/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')\n",
        "d6=pd.read_csv('/content/drive/MyDrive/data/cicids-2017/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')\n",
        "d7=pd.read_csv('/content/drive/MyDrive/data/cicids-2017/Wednesday-workingHours.pcap_ISCX.csv')\n",
        "d8=pd.read_csv('/content/drive/MyDrive/data/cicids-2017/Tuesday-WorkingHours.pcap_ISCX.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6hNQJkgZwJ4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28eaac08-98dc-42f9-ece2-ced7118972dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2830743, 79)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data=pd.concat([d1,d2,d3,d4,d5,d6,d7,d8])\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UB_7pDkghjM"
      },
      "outputs": [],
      "source": [
        "data=pd.concat([d1,d2,d3,d4,d5,d6,d7,d8]).drop_duplicates(keep=False)\n",
        "data.reset_index(drop=True, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH-aRKHegqGf"
      },
      "outputs": [],
      "source": [
        "del d1,d2,d3,d4,d5,d6,d7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otT4P8cFgu1U"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPArHDAvgzhB"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_faQJMbxhDeT"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pS8zTNTpZPI"
      },
      "outputs": [],
      "source": [
        "data.columns\n",
        "print(data[' Label'].unique())\n",
        "len(data[' Label'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDEs2EHupjNV"
      },
      "source": [
        "**Data Cleaning**\n",
        "This chapter contains data cleaning code. We go through the process of renaming columns, removing NaN and non-finite values (-inf, inf) to get the data ready for visualization and model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqTDm6_Wpl0R"
      },
      "outputs": [],
      "source": [
        "# Removing whitespaces in column names.\n",
        "#renommer les colonnes\n",
        "col_names = [col.replace(' ', '') for col in data.columns]\n",
        "data.columns = col_names\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiYCKmEauyc7"
      },
      "outputs": [],
      "source": [
        "data.columns\n",
        "print(data['Label'].unique())\n",
        "len(data['Label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PDhJ3Hep6IJ"
      },
      "outputs": [],
      "source": [
        "# Here we can see that 'Label' column contains some wierd characters. \n",
        "\n",
        "data[\"Label\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWqlC8Uxp8Mt"
      },
      "outputs": [],
      "source": [
        "# This next snippet uses regular expressions to replace wierd characters with dunders.\n",
        "\n",
        "label_names = data['Label'].unique()\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "label_names = [re.sub(\"[^a-zA-Z ]+\", \"\", l) for l in label_names]\n",
        "label_names = [re.sub(\"[\\s\\s]\", '_', l) for l in label_names]\n",
        "label_names = [lab.replace(\"__\", \"_\") for lab in label_names]\n",
        "\n",
        "label_names, len(label_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muNJ37j9qKA6"
      },
      "outputs": [],
      "source": [
        "# Replacing 'Label' column values with new readable values.\n",
        "\n",
        "labels = data['Label'].unique()\n",
        "\n",
        "for i in range(0,len(label_names)):\n",
        "    data['Label'] = data['Label'].replace({labels[i] : label_names[i]})  \n",
        "data['Label'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQHAn1xZqWw-"
      },
      "outputs": [],
      "source": [
        "# Checking if there are any NULL values in the dataset.\n",
        "\n",
        "data.isnull().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGFPqCJPqhV4"
      },
      "outputs": [],
      "source": [
        "# Checking which column/s contain NULL values.\n",
        "\n",
        "[col for col in data if data[col].isnull().values.any()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxZw_Fw2qoHU"
      },
      "outputs": [],
      "source": [
        "# Checking how many NULL values it this column contains.\n",
        "\n",
        "data['FlowBytes/s'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irHaVZshqrsN"
      },
      "outputs": [],
      "source": [
        "# Removing rows that contain NULL values and checking if number of removed rows is equal to the number of null values.\n",
        "\n",
        "before = data.shape\n",
        "data.dropna(inplace=True)\n",
        "after = data.shape\n",
        "before[0] - after[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdnrdOCBrMBU"
      },
      "outputs": [],
      "source": [
        "data.isnull().any().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKH_ue06rTpB"
      },
      "source": [
        "**Removing non-finite values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrFxCDEArQXH"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ardz4EDKratP"
      },
      "outputs": [],
      "source": [
        "labl = data['Label']\n",
        "data = data.loc[:, data.columns != 'Label'].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRSMBFs0rj39"
      },
      "outputs": [],
      "source": [
        "# Checking if all values are finite.\n",
        "np.all(np.isfinite(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V12Gi0DOrlfd"
      },
      "outputs": [],
      "source": [
        "# Checking what column/s contain non-finite values.\n",
        "\n",
        "nonfinite = [col for col in data if not np.all(np.isfinite(data[col]))]\n",
        "nonfinite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGz5Wbe1r4J7"
      },
      "outputs": [],
      "source": [
        "# Checking how many non-finite values each column contains.\n",
        "\n",
        "finite= np.isfinite(data['FlowBytes/s']).sum()\n",
        "data.shape[0] - finite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkLs_Plur7DR"
      },
      "outputs": [],
      "source": [
        "# Checking how many non-finite values each column contains.\n",
        "\n",
        "finite= np.isfinite(data['FlowPackets/s']).sum()\n",
        "data.shape[0] - finite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AbX98hasFnE"
      },
      "outputs": [],
      "source": [
        "# Same as before, since there is a small number of non-finite values we can safely remove them from the dataset\n",
        "# without spoiling the dataset.\n",
        "\n",
        "# Replacing infinite values with NaN values.\n",
        "data = data.replace([np.inf,-np.inf],np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9Zw6Y_ysIxp"
      },
      "outputs": [],
      "source": [
        "# We can see that now we have Nan values again.\n",
        "\n",
        "np.any(np.isnan(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyFsU6K6sM05"
      },
      "outputs": [],
      "source": [
        "# Bringing the Labels back into the dataset before deliting Nan rows.\n",
        "\n",
        "data = data.merge(labl, how='outer', left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvrgX-zksU5r"
      },
      "outputs": [],
      "source": [
        "# Removing new NaN values.\n",
        "\n",
        "before = data.shape\n",
        "data.dropna(inplace=True)\n",
        "after = data.shape\n",
        "before[0] - after[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R047vDy6sZ0c"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJUALkjts2zP"
      },
      "source": [
        "**Data preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVCAVq7ps4QI"
      },
      "outputs": [],
      "source": [
        "# Splitting dataset into features and labels.\n",
        "\n",
        "labels = data['Label']\n",
        "features = data.loc[:, data.columns != 'Label'].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc6DFQsB6AIb"
      },
      "outputs": [],
      "source": [
        "labels.head()\n",
        "i=0\n",
        "for row in labels:\n",
        "  if row!='BENIGN':\n",
        "    i+=1\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4Wqhivmtb2k"
      },
      "outputs": [],
      "source": [
        "# For scaling the data, we use RobustScaler class from sklearn.\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "features = scaler.fit_transform(features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNEzUIUMtik8"
      },
      "outputs": [],
      "source": [
        "# Checking if scaling has been succesful.\n",
        "features[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaH-rx8Ftul5"
      },
      "source": [
        "**Label encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M67IyCXLttHr"
      },
      "outputs": [],
      "source": [
        "#multiclass\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "LE = LabelEncoder()\n",
        "LE.fit(labels)\n",
        "labels = LE.transform(labels)\n",
        "# Labels have been replaced with integers.\n",
        "np.unique(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVP7RmyKVjwY"
      },
      "outputs": [],
      "source": [
        "#binary\n",
        "for i in range(len(labels)):\n",
        " if labels[i]==0:\n",
        "   labels[i]=0\n",
        " else :\n",
        "   labels[i]=1\n",
        "np.unique(labels)\n",
        "labels=pd.get_dummies(labels)\n",
        "labels = scaler.fit_transform(labels)\n",
        "labels[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPAI5X81umUV"
      },
      "source": [
        "**Splitting the data**\n",
        "Final step to data preparation is splitting the data into traning and testing sets. For this there already exists sklearn function that does all the splitting for us. This step is important so we can have representative data for evaluating our model. Both train and test samples should contain similar data variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLLL7kTJujC3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# The next step is to split training and testing data. For this we will use sklearn function train_test_split().\n",
        "# In the first step we will split the data in training and remaining dataset\n",
        "x_train, x_rem, y_train, y_rem = train_test_split(features,labels, train_size=0.8)\n",
        "x_train.shape, x_rem.shape, y_train.shape, y_rem.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeCDyw_w9g63"
      },
      "outputs": [],
      "source": [
        "x_valid, x_test, y_valid, y_test = train_test_split(x_rem,y_rem, test_size=0.5)\n",
        "x_valid.shape,x_test.shape, y_valid.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfKhfLqX-8fB"
      },
      "outputs": [],
      "source": [
        "#instancier le modéle\n",
        "from tensorflow.keras.models import Sequential\n",
        "model=Sequential()\n",
        "#Créer la couche entrée totalement connecté avec la couche dense\n",
        "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Flatten, AveragePooling1D,BatchNormalization,SpatialDropout1D,MaxPool1D\n",
        "from tensorflow.keras.layers import LeakyReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ3fzgkB_nb_"
      },
      "outputs": [],
      "source": [
        "model.add(Conv1D(32,kernel_size =5,padding=\"same\",input_shape =(x_train.shape[1],1)))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(AveragePooling1D(pool_size=2))\n",
        "\n",
        "# #2em block\n",
        "model.add(Conv1D(64,kernel_size =5,padding=\"same\",))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(AveragePooling1D(pool_size=2))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "\n",
        "#3em block\n",
        "model.add(Conv1D(128,kernel_size =5,padding=\"same\",))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(AveragePooling1D(pool_size=2))\n",
        "\n",
        "#4em block\n",
        "model.add(Conv1D(256,kernel_size =5,padding=\"same\",))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(AveragePooling1D(pool_size=2))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=20, activation=\"relu\"))\n",
        "\n",
        "#model.add(Dropout(0.3))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(2,activation=\"sigmoid\"))\n",
        "model.summary() #sert afficher résumés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7wmdbEN_s_U"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy',keras.metrics.Precision()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNuedV10_wAT"
      },
      "outputs": [],
      "source": [
        "#model.fit(x_train, y_train, epochs = 100, batch_size =1000)\n",
        "#model.fit(x_train, y_train, epochs =80,batch_size =128,validation_data=(x_valid, y_valid),callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1))\n",
        "from keras.callbacks import EarlyStopping\n",
        "#history = model.fit(x_train, y_train, epochs =80,batch_size =128,validation_data=(x_valid, y_valid),callbacks = [EarlyStopping(monitor='val_loss', patience=5)])\n",
        "history = model.fit(x_train, y_train, epochs =100,batch_size =5000,validation_data=(x_valid, y_valid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f1qF1VE_0Ma"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "loss_val = history.history['val_loss']\n",
        "acc_val = history.history['loss']\n",
        "epochs = range(1,76)\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.plot(epochs, acc_val, 'g', label='train loss')\n",
        "plt.title('Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ptrzh8Is_1oM"
      },
      "outputs": [],
      "source": [
        "acc_val = history.history['val_accuracy']\n",
        "acc_train = history.history['accuracy']\n",
        "epochs = range(1,101)\n",
        "plt.plot(epochs, acc_val, 'b', label='validation acc')\n",
        "plt.plot(epochs, acc_train, 'g', label='train_accuracy')\n",
        "plt.title('accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPFkeeTcDhIB"
      },
      "outputs": [],
      "source": [
        "#check accuaracy of prediction\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score, recall_score\n",
        "#model.score(x_valid,y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKniILRJDo_7"
      },
      "outputs": [],
      "source": [
        "#cf_matrix=confusion_matrix(y_valid, y_pred)\n",
        "y_test_arg=np.argmax(y_test1,axis=1)\n",
        "Y_pred = np.argmax(model.predict(x_test1),axis=1)\n",
        "print('Confusion Matrix')\n",
        "cf_matrix=confusion_matrix(y_test_arg, Y_pred)\n",
        "print(cf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxBcYcnaDrRZ"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.heatmap(cf_matrix, cmap='coolwarm',annot=True, linewidth=1,fmt=\"d\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIM0Ib1rDumn"
      },
      "outputs": [],
      "source": [
        "accuracy =accuracy_score(y_test_arg, Y_pred)*100\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45-4YWQJDz2e"
      },
      "outputs": [],
      "source": [
        "recall = recall_score(y_test_arg, Y_pred , average=\"binary\")\n",
        "precision = precision_score(y_test_arg, Y_pred, average=\"binary\")\n",
        "FPR = cf_matrix[0][1]/(cf_matrix[0][1]+cf_matrix[1][1]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRCgR538D3sA"
      },
      "outputs": [],
      "source": [
        "print(\"Precision : \" , precision*100)\n",
        "print(\"Recall : \", recall*100)\n",
        "print(\"Accuracy : \",accuracy)\n",
        "print(\"FPR : \",FPR*100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CNN_cicids2017.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNVICvNtXx0wD/+sbt1zJQa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}